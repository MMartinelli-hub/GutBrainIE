{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d20cfd0",
   "metadata": {},
   "source": [
    "# Named Entity Linking (NEL) Pipeline\n",
    "\n",
    "This notebook implements a comprehensive Named Entity Linking system for the GutBrainIE dataset. The pipeline uses a two-stage approach to link predicted entities to their corresponding URIs:\n",
    "\n",
    "## **Linking Strategy**\n",
    "\n",
    "1. **Exact Matching**: Direct text span matching against manually annotated training data\n",
    "2. **Similarity Matching**: Semantic similarity using PubMedBERT embeddings for entities without exact matches\n",
    "\n",
    "## **Pipeline Workflow**\n",
    "\n",
    "1. **Extract Training Knowledge**: Build entity-to-URI mappings from gold standard annotations\n",
    "2. **Analyze Coverage**: Evaluate exact match coverage on predicted entities  \n",
    "3. **Semantic Similarity**: Use embedding-based similarity for missed entities\n",
    "4. **Final Linking**: Combine exact and similarity matches to assign URIs\n",
    "\n",
    "## **Output**\n",
    "- Linked entities with URI assignments and confidence sources\n",
    "- Statistics on exact matches, similarity matches, and unlinked entities\n",
    "- Final predictions in evaluation format\n",
    "\n",
    "## **Requirements**\n",
    "- GPU recommended for similarity matching (txtai + PubMedBERT)\n",
    "- Pre-generated URI definitions from `generate_definitions.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc97c31",
   "metadata": {},
   "source": [
    "## Step 1: Extract Entity-to-URI Mappings from Training Data\n",
    "\n",
    "Build exact matching dictionaries from manually annotated data (dev, train_platinum, train_gold sets). We create mappings from normalized text spans to their corresponding URIs.\n",
    "\n",
    "**Strategy**: For entities with identical text spans but different URIs, we'll use the first URI found (as noted, better handling could be implemented as future work)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a063ec",
   "metadata": {},
   "source": [
    "### Text Span to URI Mapping (Case-Insensitive)\n",
    "\n",
    "Extract all entity linkages from training data for exact matching. If a predicted entity's text span (lowercase) matches exactly with training data, we assign the corresponding URI.\n",
    "\n",
    "**Note**: Some text spans may have multiple URIs in the training data. In such cases, we use the first URI found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19f4b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Define paths to training annotation files  \n",
    "# Here we are commenting out the dev set to avoid data leakage when running inference on the dev set\n",
    "# When evaluating on the test set, uncomment this line to include the dev annotations in the knowledge base\n",
    "#dev_annotations_path = '../../Annotations/Dev/json_format/dev.json'\n",
    "train_platinum_annotations_path = '../../Annotations/Train/platinum_quality/json_format/train_platinum.json'\n",
    "train_gold_annotations_path = '../../Annotations/Train/gold_quality/json_format/train_gold.json'\n",
    "\n",
    "training_annotation_files = [\n",
    "    #dev_annotations_path, \n",
    "    train_platinum_annotations_path,\n",
    "    train_gold_annotations_path\n",
    "]\n",
    "\n",
    "# Build mapping from text spans to URIs (case-insensitive)\n",
    "text_span_to_uris = {}\n",
    "\n",
    "print(\"Processing training annotations to build text span -> URI mappings...\")\n",
    "\n",
    "for annotation_file_path in training_annotation_files:\n",
    "    with open(annotation_file_path, 'r', encoding='utf-8') as input_file:\n",
    "        annotation_data = json.load(input_file)\n",
    "\n",
    "    # Process each document's entities\n",
    "    for document_id, document_content in annotation_data.items():\n",
    "        for entity in document_content['entities']:\n",
    "            normalized_text_span = entity['text_span'].lower()\n",
    "            entity_label = entity['label']\n",
    "            entity_uri = entity['uri']\n",
    "            \n",
    "            # Initialize set for this text span if not exists\n",
    "            if normalized_text_span not in text_span_to_uris:\n",
    "                text_span_to_uris[normalized_text_span] = set()\n",
    "            \n",
    "            # Add URI to the set for this text span\n",
    "            text_span_to_uris[normalized_text_span].add(entity_uri)\n",
    "\n",
    "# Analyze text spans with multiple URIs\n",
    "ambiguous_text_spans_count = 0\n",
    "for text_span, uri_set in text_span_to_uris.items():\n",
    "    if len(uri_set) > 1:\n",
    "        print(f'Ambiguous text span: \"{text_span}\" has multiple URIs: {uri_set}')\n",
    "        ambiguous_text_spans_count += 1\n",
    "\n",
    "print(f'\\n=== Text Span Analysis ===')\n",
    "print(f'Total unique text spans: {len(text_span_to_uris)}')\n",
    "print(f'Text spans with multiple URIs: {ambiguous_text_spans_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8372eef",
   "metadata": {},
   "source": [
    "### Text Span + Label to URI Mapping\n",
    "\n",
    "Create a more precise mapping that considers both text span AND entity label. This provides additional specificity for disambiguation when the same text can represent different entity types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19e177e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build mapping from (text_span, label) tuples to URIs\n",
    "text_span_label_to_uris = {}\n",
    "\n",
    "print(\"Building (text_span, label) -> URI mappings...\")\n",
    "\n",
    "for annotation_file_path in training_annotation_files:\n",
    "    with open(annotation_file_path, 'r', encoding='utf-8') as input_file:\n",
    "        annotation_data = json.load(input_file)\n",
    "\n",
    "    # Process each document's entities\n",
    "    for document_id, document_content in annotation_data.items():\n",
    "        for entity in document_content['entities']:\n",
    "            normalized_text_span = entity['text_span'].lower()\n",
    "            entity_label = entity['label']\n",
    "            entity_uri = entity['uri']\n",
    "            \n",
    "            # Create composite key from text span and label\n",
    "            composite_key = (normalized_text_span, entity_label)\n",
    "            \n",
    "            # Initialize set for this composite key if not exists\n",
    "            if composite_key not in text_span_label_to_uris:\n",
    "                text_span_label_to_uris[composite_key] = set()\n",
    "            \n",
    "            # Add URI to the set for this composite key\n",
    "            text_span_label_to_uris[composite_key].add(entity_uri)\n",
    "\n",
    "# Analyze composite keys with multiple URIs\n",
    "ambiguous_composite_keys_count = 0\n",
    "for (text_span, label), uri_set in text_span_label_to_uris.items():\n",
    "    if len(uri_set) > 1:\n",
    "        print(f'Ambiguous: \"{text_span}\" (label: {label}) has multiple URIs: {uri_set}')\n",
    "        ambiguous_composite_keys_count += 1\n",
    "\n",
    "print(f'\\n=== Text Span + Label Analysis ===')\n",
    "print(f'Total unique (text_span, label) pairs: {len(text_span_label_to_uris)}')\n",
    "print(f'Pairs with multiple URIs: {ambiguous_composite_keys_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51143234",
   "metadata": {},
   "source": [
    "## Step 2: Analyze Exact Match Coverage on Predictions\n",
    "\n",
    "Evaluate how many predicted entities can be linked using exact matches from our training data mappings. We test both text-only and text+label matching strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ec72e2",
   "metadata": {},
   "source": [
    "### Coverage Analysis: Text + Label Matching\n",
    "\n",
    "Test exact match coverage using the more precise (text_span, label) mapping approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f7715f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load predicted entities for coverage analysis\n",
    "predictions_file_path = '../../Predictions/NER/baseline_predicted_entities_eval_format.json'\n",
    "with open(predictions_file_path, 'r', encoding='utf-8') as input_file:\n",
    "    predicted_entities_data = json.load(input_file)\n",
    "\n",
    "# Analyze coverage using (text_span, label) matching\n",
    "exact_matches_with_label = 0\n",
    "missed_entities_with_label = 0\n",
    "missed_entities_for_similarity = {}\n",
    "missed_entity_index = 0\n",
    "\n",
    "print(\"Analyzing exact match coverage with (text_span, label) approach...\")\n",
    "\n",
    "for document_id, document_content in predicted_entities_data.items():\n",
    "    for predicted_entity in document_content['entities']:\n",
    "        normalized_text_span = predicted_entity['text_span'].lower()\n",
    "        entity_label = predicted_entity['label']\n",
    "        composite_key = (normalized_text_span, entity_label)\n",
    "        \n",
    "        if composite_key in text_span_label_to_uris:\n",
    "            exact_matches_with_label += 1\n",
    "        else:\n",
    "            missed_entities_with_label += 1\n",
    "            # Store missed entity for similarity matching\n",
    "            missed_entities_for_similarity[missed_entity_index] = (\n",
    "                predicted_entity['text_span'], \n",
    "                entity_label\n",
    "            )\n",
    "            missed_entity_index += 1\n",
    "\n",
    "print(f'\\n=== Coverage Analysis (Text + Label) ===')\n",
    "print(f'Exact matches: {exact_matches_with_label}')\n",
    "print(f'Missed entities: {missed_entities_with_label}')\n",
    "print(f'Coverage rate: {(exact_matches_with_label / (exact_matches_with_label + missed_entities_with_label) * 100):.1f}%')\n",
    "\n",
    "# Save missed entities for similarity matching\n",
    "MISSED_ENTITIES_FILE = 'missed_ents_text_and_label.json'\n",
    "with open(MISSED_ENTITIES_FILE, 'w', encoding='utf-8') as output_file:\n",
    "    json.dump(missed_entities_for_similarity, output_file, indent=4)\n",
    "\n",
    "print(f'\\nSaved {len(missed_entities_for_similarity)} missed entities to {MISSED_ENTITIES_FILE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ac90ba",
   "metadata": {},
   "source": [
    "### Coverage Analysis: Text-Only Matching + Missed Entity Collection\n",
    "\n",
    "Test coverage using text-only matching (more permissive) and collect entities that cannot be matched for similarity-based linking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73771d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze coverage using text-only matching and collect missed entities\n",
    "exact_matches_text_only = 0\n",
    "missed_entities_text_only = 0\n",
    "missed_entities_for_similarity = {}\n",
    "missed_entity_index = 0\n",
    "\n",
    "print(\"Analyzing exact match coverage with text-only approach...\")\n",
    "\n",
    "for document_id, document_content in predicted_entities_data.items():\n",
    "    for predicted_entity in document_content['entities']:\n",
    "        normalized_text_span = predicted_entity['text_span'].lower()\n",
    "        entity_label = predicted_entity['label']\n",
    "        \n",
    "        if normalized_text_span in text_span_to_uris:\n",
    "            exact_matches_text_only += 1\n",
    "        else:\n",
    "            missed_entities_text_only += 1\n",
    "            # Store missed entity for similarity matching\n",
    "            missed_entities_for_similarity[missed_entity_index] = (\n",
    "                predicted_entity['text_span'], \n",
    "                entity_label\n",
    "            )\n",
    "            missed_entity_index += 1\n",
    "\n",
    "print(f'\\n=== Coverage Analysis (Text Only) ===')\n",
    "print(f'Exact matches: {exact_matches_text_only}')\n",
    "print(f'Missed entities: {missed_entities_text_only}')\n",
    "print(f'Coverage rate: {(exact_matches_text_only / (exact_matches_text_only + missed_entities_text_only) * 100):.1f}%')\n",
    "\n",
    "# Save missed entities for similarity matching\n",
    "MISSED_ENTITIES_FILE = 'missed_ents_text_only.json'\n",
    "with open(MISSED_ENTITIES_FILE, 'w', encoding='utf-8') as output_file:\n",
    "    json.dump(missed_entities_for_similarity, output_file, indent=4)\n",
    "\n",
    "print(f'\\nSaved {len(missed_entities_for_similarity)} missed entities to {MISSED_ENTITIES_FILE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a086065d",
   "metadata": {},
   "source": [
    "## Step 3: Semantic Similarity Matching\n",
    "\n",
    "For entities that couldn't be matched exactly, use semantic similarity with PubMedBERT embeddings to find the most similar URI definitions from our knowledge base.\n",
    "\n",
    "**Process**:\n",
    "1. Load pre-generated URI definitions \n",
    "2. Build embedding index using txtai + PubMedBERT\n",
    "3. Query missed entities against the definition corpus\n",
    "4. Return top-10 most similar definitions for each missed entity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1274d9cd",
   "metadata": {},
   "source": [
    "### Build Embedding Index and Perform Similarity Search\n",
    "\n",
    "⚠️ **GPU Recommended**: This step uses PubMedBERT embeddings which benefit significantly from GPU acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1274d9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import txtai\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "print(\"Initializing PubMedBERT embeddings model...\")\n",
    "# Initialize txtai embeddings with PubMedBERT (biomedical domain-specific)\n",
    "embeddings_model = txtai.Embeddings(path=\"neuml/pubmedbert-base-embeddings\", content=True)\n",
    "\n",
    "# Load URI definitions for embedding indexing\n",
    "URI_DEFINITIONS_FILE = 'definitions/split_uri_definitions.json'\n",
    "with open(URI_DEFINITIONS_FILE, 'r', encoding='utf-8') as input_file:\n",
    "    uri_definitions_data = json.load(input_file)\n",
    "\n",
    "print(f\"Loaded {len(uri_definitions_data)} URI definitions\")\n",
    "\n",
    "if os.path.exists(\"embeddings_index\"):\n",
    "    print(\"Embeddings index already exists. Loading existing index...\")\n",
    "    embeddings_model.load(\"embeddings_index\")\n",
    "else:\n",
    "    # Prepare data for indexing: (id, definition_text) tuples\n",
    "    definitions_for_indexing = []\n",
    "    for definition_id, definition_text in uri_definitions_data.items():\n",
    "        definitions_for_indexing.append((definition_id, definition_text))\n",
    "\n",
    "    print(\"Building embedding index... (this may take several minutes)\")\n",
    "    # Build the embedding index\n",
    "    embeddings_model.index(definitions_for_indexing)\n",
    "\n",
    "    # Save the index for future use\n",
    "    print(\"Saving embedding index...\")\n",
    "    embeddings_model.save(\"embeddings_index\") # Save in directory format\n",
    "    embeddings_model.save(\"embeddings_index.tar.gz\") # Also save as tar.gz for compatibility\n",
    "\n",
    "# Load missed entities for similarity matching\n",
    "MISSED_ENTITIES_FILE = 'missed_ents_text_only.json'\n",
    "with open(MISSED_ENTITIES_FILE, 'r', encoding='utf-8') as input_file:\n",
    "    missed_entities_data = json.load(input_file)\n",
    "\n",
    "print(f\"Performing similarity search for {len(missed_entities_data)} missed entities...\")\n",
    "\n",
    "# Perform similarity search for each missed entity\n",
    "similarity_search_results = {}\n",
    "for entity_id, (text_span, entity_label) in tqdm(missed_entities_data.items(), desc=\"Similarity matching\"):\n",
    "    # Search for top 10 most similar definitions\n",
    "    search_results = embeddings_model.search(text_span, 10)\n",
    "    \n",
    "    similarity_search_results[entity_id] = {\n",
    "        'text_span': text_span,\n",
    "        'label': entity_label,\n",
    "        'similarity_results': search_results\n",
    "    }\n",
    "\n",
    "# Save similarity results\n",
    "SIMILARITY_RESULTS_FILE = 'similarity_matching_results.json'\n",
    "with open(SIMILARITY_RESULTS_FILE, 'w', encoding='utf-8') as output_file:\n",
    "    json.dump(similarity_search_results, output_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Similarity matching completed. Results saved to {SIMILARITY_RESULTS_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e314e89",
   "metadata": {},
   "source": [
    "## Step 4: Analyze Similarity Matching Results\n",
    "\n",
    "Process and analyze the similarity matching results to understand the quality and distribution of matches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af00973",
   "metadata": {},
   "source": [
    "### Data Format Conversion and Statistical Analysis\n",
    "\n",
    "Convert similarity results to different formats and analyze the distribution of similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d57752",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "\n",
    "def convert_json_to_pickle(json_file_path, pickle_file_path):\n",
    "    \"\"\"Convert JSON file to pickle format for faster loading.\"\"\"\n",
    "    with open(json_file_path, 'r', encoding='utf-8') as input_file:\n",
    "        data = json.load(input_file)\n",
    "    \n",
    "    with open(pickle_file_path, 'wb') as output_file:\n",
    "        pickle.dump(data, output_file)\n",
    "    \n",
    "    print(f\"Converted {json_file_path} to {pickle_file_path}\")\n",
    "\n",
    "def load_pickle_data(pickle_file_path):\n",
    "    \"\"\"Load data from pickle file.\"\"\"\n",
    "    with open(pickle_file_path, 'rb') as input_file:\n",
    "        data = pickle.load(input_file)\n",
    "    return data\n",
    "\n",
    "# Convert similarity results to pickle format\n",
    "SIMILARITY_RESULTS_JSON = 'similarity_matching_results.json'\n",
    "SIMILARITY_RESULTS_PICKLE = 'similarity_matching_results.pkl'\n",
    "\n",
    "convert_json_to_pickle(SIMILARITY_RESULTS_JSON, SIMILARITY_RESULTS_PICKLE)\n",
    "similarity_results_dict = load_pickle_data(SIMILARITY_RESULTS_PICKLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11f2ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Analyze similarity score distributions\n",
    "total_similarity_score = 0\n",
    "min_similarity_score = float('inf')\n",
    "max_similarity_score = float('-inf')\n",
    "total_entities_count = 0\n",
    "all_top_scores = []\n",
    "\n",
    "print(\"Analyzing similarity score distributions...\")\n",
    "\n",
    "# Collect top similarity scores for each entity\n",
    "for entity_id, entity_data in similarity_results_dict.items():\n",
    "    # Get the top similarity result (index 0)\n",
    "    top_similarity_score = entity_data['similarity_results'][0]['score']\n",
    "    \n",
    "    # Update statistics\n",
    "    total_similarity_score += top_similarity_score\n",
    "    min_similarity_score = min(min_similarity_score, top_similarity_score)\n",
    "    max_similarity_score = max(max_similarity_score, top_similarity_score)\n",
    "    all_top_scores.append(top_similarity_score)\n",
    "    total_entities_count += 1\n",
    "\n",
    "# Sort scores for percentile analysis\n",
    "all_top_scores.sort()\n",
    "\n",
    "# Calculate average\n",
    "average_similarity_score = total_similarity_score / total_entities_count if total_entities_count > 0 else 0\n",
    "\n",
    "print(f\"\\n=== Similarity Score Analysis ===\")\n",
    "print(f\"Total entities analyzed: {total_entities_count}\")\n",
    "\n",
    "# Show distribution of scores at different thresholds\n",
    "print(f\"\\n=== Score Distribution Analysis ===\")\n",
    "for threshold_percent in range(10, 100, 10):\n",
    "    threshold_value = threshold_percent / 100\n",
    "    entities_below_threshold = sum(1 for score in all_top_scores if score < threshold_value)\n",
    "    percentage_below = (entities_below_threshold / total_entities_count) * 100 if total_entities_count > 0 else 0\n",
    "    \n",
    "    print(f\"Entities with similarity < {threshold_value:.2f}: {entities_below_threshold} ({percentage_below:.2f}%)\")\n",
    "\n",
    "print(f\"\\n=== Summary Statistics ===\")\n",
    "pprint({\n",
    "    \"average_similarity_score\": round(average_similarity_score, 4),\n",
    "    \"min_similarity_score\": round(min_similarity_score, 4),\n",
    "    \"max_similarity_score\": round(max_similarity_score, 4),\n",
    "    \"median_score\": round(all_top_scores[len(all_top_scores)//2], 4) if all_top_scores else 0\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5232fd00",
   "metadata": {},
   "source": [
    "### Process Similarity Results for Entity Linking\n",
    "\n",
    "Extract the top similarity match for each entity and prepare the mapping for final linking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abb352f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load similarity results\n",
    "SIMILARITY_RESULTS_FILE = 'similarity_matching_results.json'\n",
    "with open(SIMILARITY_RESULTS_FILE, 'r', encoding='utf-8') as input_file:\n",
    "    similarity_results = json.load(input_file)\n",
    "\n",
    "# Create mapping from text spans to top similarity match definition IDs\n",
    "text_span_to_definition_id = {}\n",
    "\n",
    "print(\"Processing similarity results to extract top matches...\")\n",
    "\n",
    "for entity_id, entity_data in similarity_results.items():\n",
    "    entity_text_span = entity_data['text_span']\n",
    "    top_similarity_result = entity_data['similarity_results'][0]  # Get top match\n",
    "    top_definition_id = top_similarity_result['id']\n",
    "    \n",
    "    # Map text span to the ID of the most similar definition\n",
    "    text_span_to_definition_id[entity_text_span] = top_definition_id\n",
    "\n",
    "# Save processed similarity mappings\n",
    "PROCESSED_SIMILARITY_FILE = \"processed_similarity_res.json\"\n",
    "with open(PROCESSED_SIMILARITY_FILE, \"w\", encoding=\"utf-8\") as output_file:\n",
    "    json.dump(text_span_to_definition_id, output_file, indent=2)\n",
    "\n",
    "print(f\"Processed similarity mappings for {len(text_span_to_definition_id)} entities\")\n",
    "print(f\"Saved to {PROCESSED_SIMILARITY_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4ed280",
   "metadata": {},
   "source": [
    "## Step 5: Final Entity Linking\n",
    "\n",
    "Combine exact matching and similarity matching to assign URIs to all predicted entities. The linking priority is:\n",
    "\n",
    "1. **Exact Match**: Direct text span match from training data\n",
    "2. **Similarity Match**: Best semantic match from embedding search  \n",
    "3. **No Match**: Assign 'NA' if neither method finds a suitable URI\n",
    "\n",
    "Each entity will be tagged with its URI source for transparency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649f81b3",
   "metadata": {},
   "source": [
    "### Apply Linking Strategy to All Predicted Entities\n",
    "\n",
    "Execute the complete linking pipeline on all predicted entities and generate the final output with linking statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c369db1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize linking statistics counters\n",
    "exact_match_count = 0\n",
    "similarity_match_count = 0\n",
    "no_match_count = 0\n",
    "\n",
    "# Load processed similarity mappings\n",
    "PROCESSED_SIMILARITY_FILE = 'processed_similarity_res.json'\n",
    "with open(PROCESSED_SIMILARITY_FILE, 'r', encoding='utf-8') as input_file:\n",
    "    text_span_to_similarity_definition_id = json.load(input_file)\n",
    "\n",
    "# Load definition ID to URI mapping\n",
    "DEFINITION_ID_TO_URI_FILE = \"definitions/id_to_uri.json\"\n",
    "with open(DEFINITION_ID_TO_URI_FILE, \"r\", encoding=\"utf-8\") as input_file:\n",
    "    definition_id_to_uri = json.load(input_file)\n",
    "\n",
    "print(\"Loading training data for exact matching...\")\n",
    "\n",
    "# Rebuild exact matching dictionary (text span -> URI) from training data\n",
    "# Here we are commenting out the dev set to avoid data leakage when running inference on the dev set\n",
    "# When evaluating on the test set, uncomment this line to include the dev annotations in the knowledge base\n",
    "#dev_annotations_path = '../../Annotations/Dev/json_format/dev.json'\n",
    "train_platinum_annotations_path = '../../Annotations/Train/platinum_quality/json_format/train_platinum.json'\n",
    "train_gold_annotations_path = '../../Annotations/Train/gold_quality/json_format/train_gold.json'\n",
    "\n",
    "training_annotation_files = [\n",
    "    #dev_annotations_path, \n",
    "    train_platinum_annotations_path, \n",
    "    train_gold_annotations_path\n",
    "]\n",
    "\n",
    "# Rebuild exact matching mappings\n",
    "exact_text_span_to_uris = {}\n",
    "for annotation_file_path in training_annotation_files:\n",
    "    with open(annotation_file_path, 'r', encoding='utf-8') as input_file:\n",
    "        annotation_data = json.load(input_file)\n",
    "\n",
    "    for document_id, document_content in annotation_data.items():\n",
    "        for entity in document_content['entities']:\n",
    "            normalized_text_span = entity['text_span'].lower()\n",
    "            entity_uri = entity['uri']\n",
    "            \n",
    "            if normalized_text_span not in exact_text_span_to_uris:\n",
    "                exact_text_span_to_uris[normalized_text_span] = set()\n",
    "            exact_text_span_to_uris[normalized_text_span].add(entity_uri)\n",
    "\n",
    "# Load predicted entities for final linking\n",
    "predictions_file_path = \"../../Predictions/NER/baseline_predicted_entities_eval_format.json\" \n",
    "with open(predictions_file_path, \"r\", encoding=\"utf-8\") as input_file:\n",
    "    final_predictions = json.load(input_file)\n",
    "\n",
    "print(\"Applying entity linking to all predicted entities...\")\n",
    "\n",
    "# Apply linking strategy to each predicted entity\n",
    "for document_id, document_content in final_predictions.items():\n",
    "    for predicted_entity in document_content['entities']:\n",
    "        entity_text_span = predicted_entity['text_span']\n",
    "        normalized_text_span = entity_text_span.lower()\n",
    "        \n",
    "        # Strategy 1: Try exact matching first\n",
    "        if normalized_text_span in exact_text_span_to_uris:\n",
    "            exact_match_count += 1\n",
    "            # Use first URI from the set (as noted in original approach)\n",
    "            assigned_uri = list(exact_text_span_to_uris[normalized_text_span])[0]\n",
    "            predicted_entity['uri'] = assigned_uri\n",
    "            predicted_entity['uri_source'] = 'exact_match'\n",
    "            \n",
    "        # Strategy 2: Try similarity matching\n",
    "        elif entity_text_span in text_span_to_similarity_definition_id:\n",
    "            similarity_match_count += 1\n",
    "            definition_id = text_span_to_similarity_definition_id[entity_text_span]\n",
    "            assigned_uri = definition_id_to_uri[definition_id]\n",
    "            predicted_entity['uri'] = assigned_uri\n",
    "            predicted_entity['uri_source'] = 'similarity_match'\n",
    "            \n",
    "        # Strategy 3: No match found\n",
    "        else:\n",
    "            no_match_count += 1\n",
    "            predicted_entity['uri'] = 'NA'\n",
    "            predicted_entity['uri_source'] = 'no_match'\n",
    "\n",
    "# Display final linking statistics\n",
    "total_entities = exact_match_count + similarity_match_count + no_match_count\n",
    "print(f'\\n=== Final Entity Linking Results ===')\n",
    "print(f'Total entities processed: {total_entities}')\n",
    "print(f'Exact matches: {exact_match_count} ({(exact_match_count/total_entities*100):.1f}%)')\n",
    "print(f'Similarity matches: {similarity_match_count} ({(similarity_match_count/total_entities*100):.1f}%)')\n",
    "print(f'No matches (NA): {no_match_count} ({(no_match_count/total_entities*100):.1f}%)')\n",
    "print(f'Overall linking rate: {((exact_match_count + similarity_match_count)/total_entities*100):.1f}%')\n",
    "\n",
    "# Save final linked predictions\n",
    "FINAL_PREDICTIONS_FILE = '../../Predictions/NEL/baseline_predicted_entities_eval_format.json'\n",
    "with open(FINAL_PREDICTIONS_FILE, 'w', encoding='utf-8') as output_file:\n",
    "    json.dump(final_predictions, output_file, indent=4)\n",
    "\n",
    "print(f'\\nFinal linked predictions saved to: {FINAL_PREDICTIONS_FILE}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GBIErepro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
